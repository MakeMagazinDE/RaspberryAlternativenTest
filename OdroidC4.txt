C copy backwards                                     :   2076.3 MB/s
 C copy backwards (32 byte blocks)                    :   2090.2 MB/s
 C copy backwards (64 byte blocks)                    :   2093.2 MB/s
 C copy                                               :   3466.6 MB/s
 C copy prefetched (32 bytes step)                    :   2147.4 MB/s
 C copy prefetched (64 bytes step)                    :   3209.6 MB/s
 C 2-pass copy                                        :   2334.1 MB/s
 C 2-pass copy prefetched (32 bytes step)             :   1306.9 MB/s (0.3%)
 C 2-pass copy prefetched (64 bytes step)             :   2116.5 MB/s (0.2%)
 C fill                                               :   6275.4 MB/s
 C fill (shuffle within 16 byte blocks)               :   6272.8 MB/s
 C fill (shuffle within 32 byte blocks)               :   6276.0 MB/s
 C fill (shuffle within 64 byte blocks)               :   6276.7 MB/s
 ---
 standard memcpy                                      :   3430.5 MB/s
 standard memset                                      :   6275.9 MB/s
 ---
 NEON LDP/STP copy                                    :   3528.1 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :   2680.4 MB/s
 NEON LDP/STP copy pldl2strm (64 bytes step)          :   3443.4 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (32 bytes step)          :   2521.3 MB/s
 NEON LDP/STP copy pldl1keep (64 bytes step)          :   3568.3 MB/s
 NEON LD1/ST1 copy                                    :   3566.4 MB/s
 NEON STP fill                                        :   6282.9 MB/s
 NEON STNP fill                                       :   5979.6 MB/s (0.2%)
 ARM LDP/STP copy                                     :   3530.6 MB/s (0.1%)
 ARM STP fill                                         :   6277.6 MB/s
 ARM STNP fill                                        :   5979.1 MB/s (0.2%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    250.0 MB/s
 NEON LDP/STP 2-pass copy (from framebuffer)          :    242.0 MB/s
 NEON LD1/ST1 copy (from framebuffer)                 :     69.1 MB/s
 NEON LD1/ST1 2-pass copy (from framebuffer)          :     68.8 MB/s
 ARM LDP/STP copy (from framebuffer)                  :    133.3 MB/s
 ARM LDP/STP 2-pass copy (from framebuffer)           :    132.1 MB/s

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns
      2048 :    0.0 ns          /     0.0 ns
      4096 :    0.0 ns          /     0.0 ns
      8192 :    0.0 ns          /     0.0 ns
     16384 :    0.6 ns          /     1.0 ns
     32768 :    3.1 ns          /     5.0 ns
     65536 :    8.2 ns          /    11.4 ns
    131072 :   10.7 ns          /    14.2 ns
    262144 :   12.8 ns          /    15.5 ns
    524288 :   16.1 ns          /    19.1 ns
   1048576 :   72.0 ns          /   105.8 ns
   2097152 :  100.7 ns          /   133.3 ns
   4194304 :  115.9 ns          /   143.3 ns
   8388608 :  129.9 ns          /   154.7 ns
  16777216 :  138.1 ns          /   161.6 ns
  33554432 :  143.4 ns          /   166.5 ns
  67108864 :  146.3 ns          /   169.7 ns

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns
      2048 :    0.0 ns          /     0.0 ns
      4096 :    0.0 ns          /     0.0 ns
      8192 :    0.0 ns          /     0.0 ns
     16384 :    0.6 ns          /     1.0 ns
     32768 :    3.3 ns          /     5.0 ns
     65536 :    8.2 ns          /    11.4 ns
    131072 :   10.7 ns          /    14.2 ns
    262144 :   12.7 ns          /    15.5 ns
    524288 :   16.2 ns          /    19.2 ns
   1048576 :   72.7 ns          /   108.5 ns
   2097152 :  100.8 ns          /   133.3 ns
   4194304 :  115.1 ns          /   142.5 ns
   8388608 :  122.2 ns          /   145.8 ns
  16777216 :  125.5 ns          /   147.0 ns
  33554432 :  127.0 ns          /   147.5 ns
  67108864 :  127.6 ns          /   147.7 ns
  
  
  
  You have chosen to measure elapsed time instead of user CPU time.
Doing aes-128 cbc for 3s on 16 size blocks: 13039759 aes-128 cbc's in 3.00s
Doing aes-128 cbc for 3s on 64 size blocks: 3539115 aes-128 cbc's in 3.00s
Doing aes-128 cbc for 3s on 256 size blocks: 910945 aes-128 cbc's in 3.00s
Doing aes-128 cbc for 3s on 1024 size blocks: 229205 aes-128 cbc's in 3.00s
Doing aes-128 cbc for 3s on 8192 size blocks: 28699 aes-128 cbc's in 3.00s
Doing aes-128 cbc for 3s on 16384 size blocks: 14340 aes-128 cbc's in 3.00s
OpenSSL 1.1.1f 31 Mar 2020
built on: Tue May 3 17:49:36 2022 UTC
options:bn(64,64) rc4(char) des(int) aes(partial) blowfish(ptr)
compiler: gcc -fPIC -pthread -Wa,--noexecstack -Wall -Wa,--noexecstack -g -O2 -fdebug-prefix-map=/build/openssl-IlmN2J/openssl-1.1.1f=. -fstack-protector-strong -Wformat -Werror=format-security -DOPENSSL_TLS_SECURITY_LEVEL=2 -DOPENSSL_USE_NODELETE -DOPENSSL_PIC -DOPENSSL_CPUID_OBJ -DOPENSSL_BN_ASM_MONT -DSHA1_ASM -DSHA256_ASM -DSHA512_ASM -DKECCAK1600_ASM -DVPAES_ASM -DECP_NISTZ256_ASM -DPOLY1305_ASM -DNDEBUG -Wdate-time -D_FORTIFY_SOURCE=2
The 'numbers' are in 1000s of bytes per second processed.
type 16 bytes 64 bytes 256 bytes 1024 bytes 8192 bytes 16384 bytes
aes-128 cbc 69545.38k 75501.12k 77733.97k 78235.31k 78367.40k 78315.52k


odroid@odroid:~/tinymembench$ sudo hdparm -Tv /dev/mmcblk1p2
/dev/mmcblk1p2:
HDIO_DRIVE_CMD(identify) failed: Invalid argument
readonly = 0 (off)
readahead = 256 (on)
HDIO_DRIVE_CMD(identify) failed: Invalid argument
geometry = 969808/4/16, sectors = 62067713, start = 264192
Timing cached reads: 2274 MB in 2.00 seconds = 1137.26 MB/sec
odroid@odroid:~/tinymembench$ fdisk -l
Disk /dev/mmcblk1: 29.74 GiB, 31914983424 bytes, 62333952 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x03823826

Device Boot Start End Sectors Size Id Type
/dev/mmcblk1p1 2048 264191 262144 128M c W95 FAT32 (LBA)
/dev/mmcblk1p2 264192 62331904 62067713 29.6G 83 Linux
